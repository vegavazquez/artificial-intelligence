{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook, by [felipe.alonso@urjc.es](mailto:felipe.alonso@urjc.es)\n",
    "\n",
    "In this notebook we will analyze feature selection and PCA techniques in toy examples for understand their fundamentals.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Feature Selection](#feature_selection)\n",
    " 1. [Filter methods](#filter_methods)\n",
    " 2. [Wrapper methods](#wrapper_methods)  \n",
    " 3. [Embedded methods](#embedded_methods)\n",
    "2. [Feature extraction / dimensionality reduction](#feature_extraction)\n",
    " 1. [PCA](#pca)\n",
    "3. [References](#references)\n",
    "4. [Project Ideas](#ideas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='feature_selection'></a>\n",
    "# 1. Feature Selection\n",
    "\n",
    "The aim of feature selection is to select the optimum subset of features/variables in order to reduce the dimensionality of the dataset (and thus mitigating the potential overfitting) and improve interpretability.\n",
    "\n",
    "<a id='filter_methods'></a>\n",
    "## 1.A Filter methods \n",
    "\n",
    "Filter methods rank the input features according to a predefined relevance score. The following are examples of filters for regression and classification problems\n",
    "\n",
    "<a id='filter_regression'></a>\n",
    "### Regression problems\n",
    "\n",
    "Let's analyze [filter methods]((https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)) for regression ([f_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_regression.html#sklearn.feature_selection.f_regression), [mutual_info_regression](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_regression.html#sklearn.feature_selection.mutual_info_regression)) using a simple example:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Example 1</b>\n",
    "\n",
    "Let $x_1, x_2,x_3$ be random variables uniformly distributed within the interval $[0,1]$. Target variable $y$ depends on these variables, as follows\n",
    "\n",
    "$$y = x_1 + \\sin{(6\\pi x_2)} + 0.1\\mathcal{N}(0, 1),$$\n",
    "\n",
    "Specifically:\n",
    "\n",
    "* $y$ depends linearly on $x_1$\n",
    "* $y$ non linear dependency on $x_2$\n",
    "* $y$ does not depend on $x_3$\n",
    "\n",
    "therefore, $x_3$ is **irrelevant** for $y$. \n",
    "\n",
    "*Note: this example extracted from [here](https://scikit-learn.org/stable/auto_examples/feature_selection/plot_f_test_vs_mi.html#sphx-glr-auto-examples-feature-selection-plot-f-test-vs-mi-py)*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filter_methods import example_one\n",
    "\n",
    "x1, x2, x3, y = example_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='filter_classification'></a>\n",
    "### Classification problems\n",
    "\n",
    "Let's define the following classification problems:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Example 2</b>: linearly separable problem.\n",
    "    \n",
    "Let $\\{\\mathbf{x}_1, \\mathbf{x}_2,\\ldots, \\mathbf{x}_5\\}$ be a set of random variables, such that\n",
    "    \n",
    "- $\\mathbf{x}_1 = z+ \\mathcal{N}(0,\\sigma_1), \\quad z \\in \\{-\\mu,+\\mu\\}$ \n",
    "- $\\mathbf{x}_2 = -3\\mathbf{x}_1 + \\mathcal{N}(0,\\sigma_2)$\n",
    "- $\\mathbf{x}_3 = \\mathcal{N}(0,2)$\n",
    "- $\\mathbf{x}_4 = \\mathcal{U}(0,1)$\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filter_methods import example_two\n",
    "\n",
    "X, y = example_two(mu = 2.5, sigma_1 = 2, sigma_2 = 5)\n",
    "\n",
    "# Uncomment this and see the differences\n",
    "\n",
    "#X, y = example_two(mu = 4.5, sigma_1 = 2, sigma_2 = 5)\n",
    "#X, y = example_two(mu = 0.5, sigma_1 = 2, sigma_2 = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "<b>Question 1</b>: What is the best separating (hyper)plane?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter methods for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filter_methods import filter_methods_classification\n",
    "\n",
    "filter_methods_classification(X, y, feat_names = ['$x_1$','$x_2$','$x_3$','$x_4$'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, note here that the **scores have been normalized** so you must be careful when comparing different situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Example 3</b>: Non-linear separable problem.\n",
    "    \n",
    "Let $\\{\\mathbf{x}_1, \\mathbf{x}_2,\\ldots, \\mathbf{x}_{24}\\}$ be a set of random variables, such that\n",
    "    \n",
    "- $\\mathbf{x}_1$ and $\\mathbf{x}_2$ define an XOR classification problem \n",
    "- $\\mathbf{x}_3 = 3(\\mathbf{x}_1+\\mathbf{x}_2)+ \\mathcal{N}(0,2)$\n",
    "- $\\mathbf{x}_4 = 2\\sqrt{\\mathbf{x}_1+\\mathbf{x}_2} + \\mathcal{N}(0,2)$\n",
    "- $\\mathbf{x}_i = \\mathcal{N}(0,4),\\quad\\forall i=5,\\ldots,24$    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.filter_methods import example_three\n",
    "\n",
    "X, y = example_three(mu=2, sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = ['$x_{'+ str(i+1) + '}$' for i in range(24)]\n",
    "filter_methods_classification(X, y, feat_names = feat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "<b>Question 2</b>: Which are the most important features?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pima Indian Diabetes Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Example 4</b>: Real classification problem\n",
    "\n",
    "Apply filter methods to Pima Indian Diabetes dataset\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ddbb import load_pima_indian\n",
    "X, y = load_pima_indian('./data/pima_indian_diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply filter methods\n",
    "filter_methods_classification(X.values, y.values, feat_names = X.columns, rotation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit a KNN algorithm on a selection of features ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Customized function\n",
    "from src.ml_utils import analyze_train_test_performance\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42, stratify=y)\n",
    "\n",
    "# We use a pipeline here\n",
    "my_model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SelectKBest(mutual_info_classif, k=3), # you might want to try f_classif\n",
    "    GridSearchCV(KNeighborsClassifier(), {'n_neighbors': range(1,25)}, cv=3),\n",
    ")\n",
    "my_model.fit(X_train, y_train)\n",
    "\n",
    "# Analyze performance\n",
    "analyze_train_test_performance(my_model,X_train,X_test,y_train,y_test)\n",
    "\n",
    "# Selected features\n",
    "features = X.columns\n",
    "mask = my_model.named_steps['selectkbest'].get_support()\n",
    "selected_features = features[mask].values\n",
    "\n",
    "print('Selected features: ', selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='wrapper_methods'></a>\n",
    "## 1.B Wrapper methods\n",
    "\n",
    "Wrappers utilize a machine learning algorithm of interest as a **black box** to score subsets of variables according to their predictive power. When estimating the number of selected features the normally use **backward/forward** procedures together with **cross-validation** techniques to assess the performance of each subset. \n",
    "\n",
    "We are using:\n",
    "\n",
    "- [Sequential Feature Selector](http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/), by [mlextend](http://rasbt.github.io/mlxtend/). See the [documentation](http://rasbt.github.io/mlxtend/installation/) page for installing\n",
    "\n",
    "\n",
    "```python\n",
    "pip install mlextend\n",
    "```\n",
    "\n",
    "\n",
    "There is a sklearn option:\n",
    "- [SequentialFeatureSelection](https://scikit-learn.org/stable/modules/feature_selection.html#sequential-feature-selection) which has been recently released but it does not provide as many information as mlextend. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "# Load the data\n",
    "X, y = load_pima_indian('./data/pima_indian_diabetes.csv')\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=2, stratify=y)\n",
    "\n",
    "# Standarization\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm  = scaler.transform(X_test)\n",
    "\n",
    "# Feature Selection analysis\n",
    "knn = KNeighborsClassifier(n_neighbors=23)\n",
    "sfs = SFS(knn, \n",
    "          k_features=X.shape[1], \n",
    "          forward=True, \n",
    "          scoring='accuracy',\n",
    "          cv=3)\n",
    "\n",
    "sfs = sfs.fit(X_train_norm, y_train, custom_feature_names=X.columns)\n",
    "\n",
    "\n",
    "# Plotting the results\n",
    "fig1 = plot_sfs(sfs.get_metric_dict(), kind='std_dev', ylabel = 'Accuracy')\n",
    "plt.ylim([0.65, 0.85])\n",
    "plt.title('Sequential Forward Selection')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame.from_dict(sfs.get_metric_dict()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Selected Feature Subset For Making New Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=23)\n",
    "sfs = SFS(knn, \n",
    "          k_features=6, \n",
    "          forward=True, \n",
    "          scoring='accuracy',\n",
    "          cv=3)\n",
    "\n",
    "sfs = sfs.fit(X_train_norm, y_train, custom_feature_names=X.columns)\n",
    "\n",
    "print('Selected features:', sfs.k_feature_names_)\n",
    "\n",
    "X_train_fs = sfs.transform(X_train_norm)\n",
    "X_test_fs  = sfs.transform(X_test_norm)\n",
    "\n",
    "# Fit the estimator using the new feature subset\n",
    "# and make a prediction on the test data\n",
    "knn_fs = knn.fit(X_train_fs, y_train)\n",
    "\n",
    "analyze_train_test_performance(knn_fs,X_train_fs,X_test_fs,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Backward Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "<b>Question 3</b>: Implement a SBS process\n",
    "    \n",
    "- `k_features = 1`\n",
    "- `forward = False`\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Feature Selection and GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "sfs = SFS(estimator=KNeighborsClassifier(), \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           scoring='accuracy',\n",
    "           cv=3)\n",
    "\n",
    "pipe = Pipeline([('sfs', sfs), \n",
    "                 ('knn', knn)])\n",
    "\n",
    "param_grid = [{\n",
    "    'sfs__k_features': range(1,9),\n",
    "    'sfs__estimator__n_neighbors': [15,20,25]\n",
    "}]\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe, \n",
    "                  param_grid=param_grid, \n",
    "                  scoring='accuracy', \n",
    "                  cv=3)\n",
    "\n",
    "# run gridsearch\n",
    "gs = gs.fit(X_train_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score:', gs.best_score_)\n",
    "print(\"Best parameters via GridSearch\", gs.best_params_)\n",
    "\n",
    "pipe.set_params(**gs.best_params_).fit(X_train_norm, y_train)\n",
    "analyze_train_test_performance(knn_fs,X_train_norm,X_test_norm,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.C Embedded Methods\n",
    "\n",
    "Embedded methods perform feature selection during the modelling algorithm's execution. Common embedded methods include Decision Trees, Random Forest, Lasso, Support Vector Machines, among others.\n",
    "\n",
    "\n",
    "We will be using \n",
    "- [SelectFromModel](https://scikit-learn.org/stable/modules/feature_selection.html#select-from-model) implemented by sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An extended feature selection method among practitioners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from src.ml_utils import plot_importances\n",
    "\n",
    "# Load the data\n",
    "X, y = load_pima_indian('./data/pima_indian_diabetes.csv')\n",
    "\n",
    "clf = RandomForestClassifier().fit(X, y)\n",
    "plot_importances(clf.feature_importances_, X.columns)\n",
    "\n",
    "# Selection\n",
    "selector = SelectFromModel(clf).fit(X,y)\n",
    "print('- Selected features: ', X.columns[selector.get_support()])\n",
    "print('- Threshold (mean importance): ', selector.threshold_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42, stratify=y)\n",
    "\n",
    "# your code here\n",
    "# ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "<b>Question 4</b>: Implement a pipeline that:\n",
    "    \n",
    "- Select features based on RandomForest feature importance\n",
    "- Applies a Logistic Regression classifier\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination\n",
    "\n",
    "Recursive Feature Elimination ([original paper](http://clopinet.com/isabelle/Papers/geneselect.pdf)): select features by recursively considering smaller and smaller sets of features based on `coef_` or `feature_importances_` attributes. \n",
    "\n",
    "In scikit-learn:\n",
    "-  [feature_selection.RFE](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE)\n",
    "-  [feature_selection.RFECV](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFECV.html#sklearn.feature_selection.RFECV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "rfecv = RFECV(\n",
    "    estimator=RandomForestClassifier(random_state=0),\n",
    "    step=1,\n",
    "    cv=3,\n",
    "    scoring=\"accuracy\",\n",
    "    min_features_to_select=1,\n",
    ")\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (accuracy)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1),rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='feature_extraction'></a>\n",
    "# 2. Feature Extraction / dimensionality reduction\n",
    "\n",
    "We will be working with the life expectancy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ddbb import load_life_expectancy\n",
    "\n",
    "X, y = load_life_expectancy('./data/Life Expectancy Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pca'></a>\n",
    "## 2.1 PCA\n",
    "\n",
    "Let's start first with an easy example\n",
    "\n",
    "*Note: this example was extracted from [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html). I encourage you to take a depth look at this notebook for understading PCA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pca_utils import toy_example_pca\n",
    "\n",
    "X = toy_example_pca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that PCA transforms (linearly) the original space into a an orthonormal basis (principal components) in which different individual dimensions of the data are linearly uncorrelated:\n",
    " - [Principal components](https://en.wikipedia.org/wiki/Principal_component_analysis#Quantitative_finance) are are a sequence of direction vectors, where the $i^{{\\text{th}}}$ vector is the direction of a line that best fits the data while being orthogonal to the first  $i-1$ vectors\n",
    " - Principal components are eigenvectors of the data's covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pca_utils import plot_pca_toy_example\n",
    "\n",
    "pca = plot_pca_toy_example(X)\n",
    "\n",
    "print('Components:\\n', pca.components_)\n",
    "print('Variance of each axis (largest eigenvalues):', pca.explained_variance_)\n",
    "print('%Variance for each axis:', pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for dimensionality reduction\n",
    "\n",
    "Project our 2D data into 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=1).fit(X)\n",
    "X_pca = pca.transform(X) # this X is on the transformed space ...\n",
    "\n",
    "print(\"original shape:   \", X.shape)\n",
    "print(\"transformed shape:\", X_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the transformed data in the original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c = 'b', alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], c='r', alpha=0.5)\n",
    "plt.xlabel('$x_1$', fontsize=16)\n",
    "plt.ylabel('$x_2$', fontsize=16)\n",
    "plt.axis('equal');\n",
    "\n",
    "print('This transformation retains the {:.2f}% of the variance'.format(100*pca.explained_variance_ratio_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Example 6</b>: Apply PCA to Pima Indian Database data\n",
    "\n",
    "- How many components do we need? \n",
    "- Visualize your data\n",
    "- Train a ML model using the transformed features\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "X, y = load_pima_indian('./data/pima_indian_diabetes.csv')\n",
    "\n",
    "# scaling\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Number of components\n",
    "pca = PCA().fit(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(1,X.shape[1]+1),np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "# Data visualization (just 2 components)\n",
    "X_pca = PCA(n_components=2).fit_transform(X_scaled)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(X_pca[:,0],X_pca[:,1],c=-1*y, cmap=cm_bright, alpha=0.5)\n",
    "plt.xlabel('$x_1$ (PCA)',fontsize=16)\n",
    "plt.ylabel('$x_2$ (PCA)',fontsize=16)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainability\n",
    "\n",
    "Even if these two variables look like promising, **do they provide with any interpretation about the data?** You might find of interest the following:\n",
    "\n",
    "- [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/)\n",
    "- [Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)\n",
    "- [The Building Blocks of Interpretability](https://distill.pub/2018/building-blocks/) (deep learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just a quick view, not a proper training\n",
    "lr =  LogisticRegression().fit(X_pca, y)\n",
    "print('ACC: {:.3f}'.format(lr.score(X_pca, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Just a quick view, not a proper training\n",
    "acc_score = [] \n",
    "for i in range(1,X.shape[1]+1):\n",
    "    X_pca = PCA(n_components=i).fit_transform(X_scaled)\n",
    "    lr =  LogisticRegression().fit(X_pca, y)\n",
    "    acc_score.append(lr.score(X_pca, y))\n",
    "\n",
    "plt.plot(np.arange(X.shape[1])+1,acc_score)\n",
    "plt.xlabel('# components')\n",
    "plt.ylabel('ACC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='references'></a>\n",
    "# References\n",
    "\n",
    "[1] [A review of feature selection methods with applications](https://bib.irb.hr/datoteka/763354.MIPRO_2015_JovicBrkicBogunovic.pdf)\n",
    "\n",
    "[2] [Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html) by sklearn\n",
    "\n",
    "[3] [MLxtend Documentation](http://rasbt.github.io/mlxtend/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='ideas'></a>\n",
    "# Project Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking into account the classic machine learning pipeline, here there are some ideas that you might want to consider for your project:\n",
    "\n",
    "1. Is it worth transforming and/or creating new features based on the data distribution or your domain knowledge? Justify your answer. \n",
    "    - Comment on variable(s) to be transfomed\n",
    "    - Comment on variable(s) to be created\n",
    "\n",
    "\n",
    "2. Is it worth performing feature selection in your dataset? Justify your answer. If you decide to perform feature selection, you might want to ...\n",
    "    - ... analyze the relationship between predictive features and the output using filter methods\n",
    "    - ... analyze feature importance using `feature_importances_` \n",
    "    - ... use filter, wrapper or embedded methods to filter out irrelevant features\n",
    "\n",
    "\n",
    "3. Is it worth applying PCA to your dataset? Justify your answer. If you decide to apply PCA, you might want to ...\n",
    "    - ... discuss on `n_components` to be using\n",
    "    - ... analyze the performance of your ML pipeline when using PCA\n",
    "    - ... visualize you data by 2D/3D plots trying to provide any explanations to your problem\n",
    "\n",
    "\n",
    "4. Is the overall performance (comparing train and test set) good enough? If not, you might want to iterate again over steps 1, 2 and 3. If you do, comment on this. \n",
    "\n",
    "In all above, justify your decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
