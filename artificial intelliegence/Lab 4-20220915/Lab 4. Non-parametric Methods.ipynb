{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook, by [felipe.alonso@urjc.es](mailto:felipe.alonso@urjc.es)\n",
    "\n",
    "In this notebook we will analyze and compare different non-parametric methods over the `pima_indian_diabetes` dataset. Specifically, we will learn:\n",
    "\n",
    "- How to train $K-$NN, DTs, Random Forest, Gradient Boosting and MLP algorithms.\n",
    "- How to set their hyperparameters (model selection)\n",
    "- How to evaluate and compare their performance (model evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/'\n",
    "filename = 'pima_indian_diabetes.csv'\n",
    "df = pd.read_csv(path + filename)\n",
    "\n",
    "# take a look to the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model Selection and evaluation on $K-$NN\n",
    "\n",
    "We will focus first on the **classification task**, using `Outcome` as target\n",
    "\n",
    "First, we split our data, but we do it wisely in order build a test set as similar as the train set. Since our target variables is *imbalaced* we might want to activate the `stratify` option.\n",
    "\n",
    "Take a look to the function documentation: [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html?highlight=train_test_split#sklearn.model_selection.train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(['Outcome'], axis=1), df['Outcome'], \n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    stratify=df['Outcome']\n",
    ")\n",
    "\n",
    "print('- Train size:', X_train.shape)\n",
    "print('- Test size:', X_test.shape)\n",
    "\n",
    "print('\\n- Train target distribution: ', y_train.value_counts().values/len(y_train))\n",
    "print('- Test target distribution:  ',y_test.value_counts().values/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Run the above cell several times, does the target variable distribution change?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarize/normalize variables\n",
    "\n",
    "$K-$NN and MLP require the input data to be standarized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "X_test_norm  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Let's set a baseline\n",
    "\n",
    "Before running our algorithms it is always a good practice to build a baseline model, so you we have a reference of what the results should be.\n",
    "\n",
    "- In `src.utils` you have implemented some functions to help us calculating and representing different classification metrics. \n",
    "\n",
    "- The `analyze_train_test_performance` function provides a comparative summary between training and test metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from src.utils import analyze_train_test_performance  \n",
    "\n",
    "# Logistic regression\n",
    "lr_model = LogisticRegression().fit(X_train_norm,y_train)\n",
    "\n",
    "# This a custom function, take a look in src.utils\n",
    "analyze_train_test_performance(lr_model,X_train_norm,X_test_norm,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Does this model overfit? Justify your answer\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Model selection: *GridSearch* \n",
    "\n",
    "We are going to sweep different values of the parameters of each algorithm, to determine its optimal value. In this sweep, we will use a cross-validation strategy, but never the test set!\n",
    "\n",
    "To do so, we will be using the [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# These are customized functions: should be in utils.py\n",
    "def hyper_parameters_search(clf, X, y, param_grid, scorer = 'f1', cv=5):\n",
    "    \n",
    "    grid = GridSearchCV(clf, param_grid = param_grid, scoring = scorer, cv = cv)\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "    print(\"best parameters: {}\".format(grid.best_params_))\n",
    "    \n",
    "    return grid\n",
    "\n",
    "def plot_cv_scoring(grid, hyper_parameter, scorer = 'f1', plot_errors = False, log=False):\n",
    "    \n",
    "    scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "    std_scores = grid.cv_results_['std_test_score']\n",
    "        \n",
    "    params = grid.param_grid[hyper_parameter]\n",
    "    \n",
    "    if log:\n",
    "        params = np.log10(params)\n",
    "    \n",
    "    if plot_errors:\n",
    "        plt.errorbar(params,scores,yerr=std_scores, fmt='o-',ecolor='g')\n",
    "    else:\n",
    "        plt.plot(params,scores, 'o-')\n",
    "    plt.xlabel(hyper_parameter,fontsize=14)\n",
    "    plt.ylabel(scorer)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train an algorithm using `GridSearchCV`\n",
    "\n",
    "We need to define:\n",
    "\n",
    "- `scoring`: strategy to evaluate the performance of the cross-validated model on the validation sets. [Metrics in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "Take a look to the different metrics in sklearn\n",
    "</div>\n",
    "\n",
    "- `param_grid`: dictionary with parameters names (`str`) as keys and lists of parameter settings to try as value. Example:\n",
    "\n",
    "```python\n",
    "param_grid = {'n_neighbors': range(1,25)}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $K-$NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Metric for the scoring\n",
    "scorer = 'accuracy'  # Other possibilities: accuracy, balanced_accuracy, f1, roc_auc, ....\n",
    "\n",
    "# param_grid\n",
    "param_grid = {'n_neighbors': range(1,25)}\n",
    "\n",
    "# Our customized function\n",
    "grid_knn = hyper_parameters_search(KNeighborsClassifier(), X_train_norm, y_train, param_grid, scorer = scorer)\n",
    "\n",
    "# do the plotting\n",
    "plot_cv_scoring(grid_knn,'n_neighbors',scorer, plot_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "\n",
    "Which is the optimal value for $k$? You might want to consider\n",
    "    \n",
    "- Different score metrics to guide the CV process<br> \n",
    "- Plot the CV errors (`plot_errors = True`) \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Model evaluation\n",
    "\n",
    "Once we have selected the model (hyper)-parameters, we evaluate its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model =  KNeighborsClassifier(**grid_knn.best_params_).fit(X_train_norm,y_train)\n",
    "\n",
    "analyze_train_test_performance(knn_model,X_train_norm,X_test_norm,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Does this model overfit? Is it better than our baseline?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Decision trees (DTs)\n",
    "\n",
    "We will repeat the above process for DTs. In this case, the hyper-parameter is `max_depth`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision trees\n",
    "param_grid = {'max_depth': range(1,15)}\n",
    "\n",
    "# your code here\n",
    "# ...\n",
    "\n",
    "scorer = 'accuracy'\n",
    "grid_dt = hyper_parameters_search(\n",
    "    DecisionTreeClassifier(random_state=0), \n",
    "    X_train_norm, y_train, param_grid, scorer = scorer, cv=5)\n",
    "\n",
    "# do the plotting\n",
    "plot_cv_scoring(grid_dt,'max_depth',scorer, plot_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# \n",
    "# dt_model =  ...\n",
    "\n",
    "dt_model =  DecisionTreeClassifier(random_state=0, max_depth=2).fit(X_train,y_train)\n",
    "\n",
    "analyze_train_test_performance(dt_model,X_train,X_test,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "\n",
    "- Does this model overfit? \n",
    "- Is it better than our baseline?\n",
    "- What if we change `max_depth`?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 DTs visualization\n",
    "\n",
    "Trees can be visualized using the [`plot_tree`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# set plot dimensions\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "feature_names=df.columns.drop('Outcome')\n",
    "\n",
    "plot_tree(\n",
    "    dt_model,\n",
    "    feature_names=feature_names, \n",
    "    class_names=['non-diabetic','diabetic'], \n",
    "    filled=True\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance\n",
    "def plot_importances(importances, feat_names):\n",
    "    \n",
    "    df_importances = pd.Series(importances, index=feat_names)\n",
    "    \n",
    "    plt.figure()\n",
    "    df_importances.plot.bar()\n",
    "    plt.ylabel(\"Feature Importance\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plot_importances(dt_model.feature_importances_, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest (RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Random Forest\n",
    "param_grid = {'max_depth': range(1,5),\n",
    "              'n_estimators' : [50,100,200,500,1000]}\n",
    "\n",
    "# 3.1.Model selection ... this might take a while\n",
    "\n",
    "# your code here\n",
    "# ...\n",
    "\n",
    "grid_rf = hyper_parameters_search(\n",
    "    RandomForestClassifier(random_state=0), \n",
    "    X_train_norm, y_train, \n",
    "    param_grid, scorer = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Evaluation\n",
    "\n",
    "# your code here\n",
    "# ...\n",
    "rf_model =  RandomForestClassifier(random_state=0, **grid_rf.best_params_).fit(X_train_norm,y_train)\n",
    "analyze_train_test_performance(rf_model,X_train_norm,X_test_norm,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Feature importance\n",
    "\n",
    "# your code here\n",
    "# ...\n",
    "plot_importances(rf_model.feature_importances_, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "\n",
    "- Does this model overfit? \n",
    "- Is it better than our baseline?\n",
    "- What are the most important features?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gradient Boosting (trees) (BT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [50,100,200,500, 1000, 2000],\n",
    "    'learning_rate': [0.1,0.05,0.01, 0.005, 0.001], \n",
    "    'max_depth': [1, 2]\n",
    "} \n",
    "\n",
    "# 4.1 Model selection ... this might take a while\n",
    "\n",
    "# your code here\n",
    "#  ...\n",
    "\n",
    "grid_bt = hyper_parameters_search(\n",
    "    GradientBoostingClassifier(random_state=0), \n",
    "    X_train_norm, y_train, param_grid, scorer='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Model evaluation\n",
    "\n",
    "# your code here\n",
    "#  ...\n",
    "\n",
    "bt_model =  GradientBoostingClassifier(random_state=0, **grid_bt.best_params_).fit(X_train_norm,y_train)\n",
    "analyze_train_test_performance(bt_model,X_train_norm,X_test_norm,y_train,y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Feature importance\n",
    "\n",
    "# your code here\n",
    "# ...\n",
    "plot_importances(bt_model.feature_importances_, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "\n",
    "- Does this model overfit? \n",
    "- Is it better than our baseline?\n",
    "- What are the most important features?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'alpha' : 10.0 ** -np.arange(1, 7), # following recommendation: https://scikit-learn.org/stable/modules/neural_networks_supervised.html#tips-on-practical-use\n",
    "    'hidden_layer_sizes': [(5,), (10,), (5,5)] # [(5 neurons, 1 hidden layer), (10 neurons, 1 hidden layer) ,...]\n",
    "} \n",
    "\n",
    "# 5.1 Model selection ...\n",
    "\n",
    "# your code here\n",
    "#  ...\n",
    "param_grid = {\n",
    "    'alpha' : 10.0 ** -np.arange(1, 7), # following recommendation: https://scikit-learn.org/stable/modules/neural_networks_supervised.html#tips-on-practical-use\n",
    "    'hidden_layer_sizes': [(5,), (10,), (5,5)] # [(5 neurons, 1 hidden layer), (10 neurons, 1 hidden layer)]\n",
    "} \n",
    "\n",
    "grid_mlp = hyper_parameters_search(\n",
    "    MLPClassifier(random_state=0), \n",
    "    X_train_norm, y_train, param_grid, scorer='accuracy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Model evaluation\n",
    "\n",
    "# your code here\n",
    "#  ...\n",
    "mlp_model =  MLPClassifier(random_state=0, **grid_mlp.best_params_).fit(X_train_norm,y_train)\n",
    "analyze_train_test_performance(mlp_model,X_train_norm,X_test_norm,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "\n",
    "- Does this model overfit? \n",
    "- Is it better than our baseline?\n",
    "- What are the most important features?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model comparison\n",
    "\n",
    "If you have done all of the above, just run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "modelos = {'LR': lr_model, 'KNN':knn_model,'DT':knn_model, 'RF': rf_model, 'BT': bt_model, 'MLP': mlp_model}\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "for k,v in modelos.items():\n",
    "    fpr, tpr,_ = roc_curve(y_test, modelos[k].predict_proba(X_test_norm)[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label = k + ': %0.2f' % roc_auc)\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "\n",
    "Taking into account the above results, in your opinion\n",
    "    \n",
    "- What is the best model? Justify your answer \n",
    "- And the most important feature(s)? \n",
    "- If you want to improve these results, what would you do?\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Your turn!\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "\n",
    "1. Apply non-parametric methods to a regression problem. \n",
    "    - Which performance metric would you use?\n",
    "\n",
    "\n",
    "2. Apply non-parametric methods to your dataset. \n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Ideas\n",
    "\n",
    "Here there are some tips that you might want to consider for your project:\n",
    "\n",
    "1. Select a baseline classifier. Which one did you choose? Why?\n",
    "\n",
    "\n",
    "2. Compare different machine learning methods. Which one provides you with the best performance? Comment on:\n",
    "    - ... how you selected the best model (hyperparameters you chose and the performance metric you used to do so)\n",
    "    - ... model overfitting\n",
    "\n",
    "\n",
    "3. If you used decision trees or decision trees based algorithms, which are the most important features? Are they coherent with you domain expertise?\n",
    "\n",
    "\n",
    "In all above, justify your decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
